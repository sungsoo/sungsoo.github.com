---
layout: post
title: SGD and Weight Decay Secretly Compress Your Neural Network
date: 2024-09-13
categories: [artificial intelligence]
tags: [machine learning]
use_math: true

---

### Article Source


* [SGD and Weight Decay Secretly Compress Your Neural Network](https://www.youtube.com/watch?v=axprAoA-Gew)

---



# SGD and Weight Decay Secretly Compress Your Neural Network

* Tomer Galanti, MIT

## ABSTRACT 


<iframe width="600" height="400" src="https://www.youtube.com/embed/axprAoA-Gew?si=vCBKAY3tyYIt-Zxi" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


## Background

**Low-Rank Bias**는 딥러닝 모델, 특히 심층 신경망(Deep Neural Networks, DNN)에서 학습된 가중치 행렬이 본질적으로 **저랭크(low-rank) 성질**을 갖는 경향을 설명하는 개념입니다. 딥러닝에서의 **랭크(rank)**는 행렬의 선형 독립성을 나타내는 개념으로, 저랭크 행렬은 그 행렬이 적은 수의 선형 독립 벡터로 표현될 수 있음을 의미합니다.

이 개념은 딥러닝에서 **모델 압축**, **일반화**, **효율성**을 설명하는 중요한 역할을 합니다. 저랭크 행렬은 모델의 복잡성을 낮추고 과적합을 방지하며, 학습된 가중치가 더 효율적인 표현을 갖도록 유도할 수 있습니다.

## Low-Rank Bias의 정의 및 개념

1. **Low-Rank Matrix (저랭크 행렬)**:
   - 행렬의 **랭크(rank)**는 행렬에 포함된 선형 독립된 행이나 열 벡터의 수를 나타냅니다. 
   - **저랭크 행렬(low-rank matrix)**는 선형 독립적인 벡터의 수가 적은 경우로, 예를 들어, $$m \times n$$ 행렬에서 선형 독립적인 벡터의 수(랭크)가 $$ \min(m, n) $$보다 작을 때, 이 행렬은 저랭크라고 부를 수 있습니다.
   - 고랭크(high-rank) 행렬은 매우 복잡하고 다양한 정보를 표현할 수 있는 반면, 저랭크 행렬은 더 제한된 정보 표현을 가집니다.

2. **Low-Rank Bias의 직관**:
   딥러닝 모델에서, 특히 완전 연결 계층(fully connected layers)과 같은 계층에서 **Low-Rank Bias**는 가중치 행렬이 학습 과정에서 자연스럽게 저랭크 구조를 취하게 된다는 현상을 말합니다. 이 현상은 모델이 데이터의 중요한 패턴이나 특징을 학습할 때, 데이터에서 학습된 가중치 행렬이 고랭크 행렬보다는 저랭크 형태로 수렴할 가능성이 높다는 것을 의미합니다.

   이는 딥러닝 모델이 데이터를 **효율적으로 압축된 표현**으로 학습하며, 불필요한 세부 정보를 제거하려는 경향이 있다는 것을 나타냅니다. 특히, 매우 복잡하고 거대한 모델이라 하더라도 실제로 학습된 가중치 행렬의 랭크는 상대적으로 낮을 수 있습니다.

## Low-Rank Bias의 기원

Low-Rank Bias는 여러 요인에 의해 발생할 수 있습니다:

1. **최적화 기법(SGD 등의 효과)**:
   - **확률적 경사 하강법(Stochastic Gradient Descent, SGD)**과 같은 최적화 기법은 전체 데이터셋을 한 번에 학습하지 않고, 배치 단위로 학습을 진행합니다. 이러한 방식은 학습 과정에서의 **노이즈**로 인해 모델이 복잡한 고랭크 표현보다는 단순한 저랭크 표현을 선호하는 방향으로 학습됩니다.
   - SGD는 저랭크 행렬에서 더 빠르고 효율적으로 수렴하는 경향이 있으며, 이로 인해 딥러닝 모델의 가중치 행렬이 자연스럽게 저랭크 형태를 취할 수 있습니다.

2. **신경망의 구조와 레이어**:
   - 심층 신경망의 구조적인 특성도 Low-Rank Bias를 유발할 수 있습니다. 예를 들어, 심층 신경망은 여러 계층(layer)을 거치며 입력 데이터를 점점 더 추상적이고 고차원적인 특징으로 변환합니다. 이러한 과정에서 고차원 데이터가 더 압축된 표현(저차원 공간)으로 변환될 수 있으며, 이는 각 레이어의 가중치 행렬이 저랭크가 되는 경향을 띠게 합니다.
   - 네트워크가 깊어질수록 정보가 여러 레이어를 거치면서 압축되기 때문에, 하위 계층에서 학습된 가중치는 종종 저랭크 구조를 갖게 됩니다.

3. **데이터의 특성**:
   - 딥러닝 모델이 처리하는 데이터 자체가 저차원 구조를 가지고 있거나, 고차원 데이터를 저차원 공간으로 임베딩할 수 있다면, 모델의 가중치가 자연스럽게 저랭크 행렬로 표현될 가능성이 높습니다.
   - 예를 들어, 고차원 자연 이미지 데이터를 학습할 때, 모델은 이미지의 저차원 잠재 구조(latent structure)를 학습할 수 있으며, 이러한 학습 과정은 모델 가중치가 저랭크 형태를 띠도록 유도합니다.

4. **규제화(Regularization)**:
   - 명시적인 규제 기법, 예를 들어 L2 정규화(L2 regularization) 또는 Dropout과 같은 기법도 Low-Rank Bias를 촉진할 수 있습니다. 규제화는 가중치의 크기를 제어함으로써 모델이 과도하게 복잡한 해결책으로 수렴하는 것을 방지하고, 더 단순한 표현을 학습하도록 유도합니다.
   - 이는 가중치 행렬의 랭크를 낮추는 방향으로 학습 과정을 유도할 수 있습니다.

## Low-Rank Bias의 영향

1. **모델 압축(Model Compression)**:
   - 저랭크 가중치 행렬은 모델의 **효율성**을 크게 향상시킬 수 있습니다. 저랭크 행렬을 이용해 원래 크기의 가중치 행렬을 근사할 수 있으며, 이는 모델 파라미터 수를 줄이고 **모델 압축**을 가능하게 합니다.
   - 예를 들어, $$W = U \Sigma V^T$$로 표현할 수 있는 가중치 행렬이 있을 때, 저랭크 근사 $$W' \approx U_r \Sigma_r V_r^T$$를 통해 가중치 행렬을 더 작은 랭크 $$r$$로 근사할 수 있습니다. 이는 **저장 공간**과 **연산 비용**을 크게 줄일 수 있습니다.

2. **일반화 성능 향상(Generalization)**:
   - 저랭크 가중치는 모델의 **일반화 성능**을 향상시킬 수 있습니다. 모델이 저랭크 형태로 학습된다는 것은 데이터의 중요한 패턴만을 학습하고, 불필요한 세부 사항(노이즈)을 무시하는 경향이 있음을 의미합니다.
   - 이는 과적합(overfitting)을 방지하는 데 도움이 되며, 새로운 데이터에 대해서도 더 잘 일반화할 수 있는 모델을 만들 수 있습니다.

3. **메모리 및 계산 효율성**:
   - 저랭크 근사를 활용하면, 딥러닝 모델의 가중치 행렬을 더 작은 랭크로 표현함으로써 **메모리 사용량**을 줄이고, **연산량**을 줄일 수 있습니다. 특히 대규모 신경망에서는 저랭크 근사와 같은 기법이 모델의 효율성을 크게 높여줄 수 있습니다.
   - 또한, 저랭크 행렬을 사용하면 네트워크를 분산 시스템에서 병렬화하기가 더 쉬워져 학습 속도도 향상될 수 있습니다.

4. **모델 압축 기법과의 연결**:
   - Low-Rank Bias는 **모델 압축 기법**과 자연스럽게 연결됩니다. 저랭크 근사를 사용하여 가중치 행렬을 줄이거나, 저랭크 가정 하에 모델을 학습하면 파라미터 수를 크게 줄일 수 있으며, 이는 메모리 및 연산량을 줄이는 압축된 딥러닝 모델을 만들 수 있습니다.

## Low-Rank Bias를 활용한 실제 응용

1. **저랭크 근사(Low-Rank Approximation)**:
   - 저랭크 근사 기법은 딥러닝에서 가중치 행렬을 저랭크로 근사함으로써 **모델 압축**을 수행하는 데 활용됩니다. 이는 파라미터 수를 줄이고, 학습 및 추론 속도를 높이는 데 도움이 됩니다.
   - 예를 들어, 원래의 가중치 행렬 $$ W $$를 저랭크 행렬 $$ W' = U_r \Sigma_r V_r^T $$로 근사함으로써, 연산 복잡도를 줄일 수 있습니다. 이는 특히 대규모 네트워크에서 매우 유용합니다.

2. **고차원 데이터의 저차원 표현 학습**:
   - 자연 이미지, 비디오, 텍스트 등 고차원 데이터의 경우, 데이터가 저차원 잠재 구조를 가지는 경우가 많습니다. Low-Rank Bias는 딥러닝 모델이 이러한 저차원 구조를 자동으로 학습하도록 돕고, 더 압축된 표현을 통해 효율적인 학습을 가능하게 합니다.
  
