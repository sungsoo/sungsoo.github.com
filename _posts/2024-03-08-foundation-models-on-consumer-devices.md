---
layout: post
title: Foundation Models on Consumer Devices
date: 2024-03-08
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source


* [Foundation Models on Consumer Devices](https://www.youtube.com/watch?v=InoNMvjs_vo&list=PLSrTvUm384I9PV10koj_cqit9OfbJXEkq&index=83)

---

# Foundation Models on Consumer Devices


* Bringing Foundational Models to Consumer Devices via ML Compilation
* Speaker: Tianqi Chen


## Abstract


Deploying deep learning models on various devices has become an important topic. Machine learning compilation is an emerging field that leverages compiler and automatic search techniques to accelerate AI models. ML compilation brings a unique set of challenges: emerging machine learning models; increasing hardware specialization brings a diverse set of acceleration primitives; growing tension between flexibility and performance.  In this talk. I then discuss our experience in bringing foundational models to a variety of devices and hardware environments through machine learning compilation.

딥러닝 모델의 다양한 기기 활용, 머신러닝 컴파일러가 열쇠
최근 딥러닝 모델을 다양한 기기에 적용하는 것은 중요한 연구 주제가 되어가고 있습니다. 머신 러닝 컴파일러는 컴파일러와 자동 검색 기술을 활용하여 AI 모델의 성능을 가속화하는 새로운 분야입니다. 하지만 머신 러닝 컴파일러는 다음과 같은 고유한 과제에 직면합니다.

* 끊임없이 새롭게 등장하는 머신 러닝 모델
* 하드웨어 특수화가 증가함에 따른 다양한 가속화 기본 기능
* 유연성과 성능 간의 갈등 증대


이 발표에서는 머신 러닝 컴파일러를 통해 기초 모델을 다양한 기기 및 하드웨어 환경에 적용하는 저희 연구 경험에 대해 이야기 드리겠습니다.


<iframe width="600" height="400" src="https://www.youtube.com/embed/InoNMvjs_vo?si=g5KykKwb2ikYC0yn" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## Bio
Tianqi Chen is currently an Assistant Professor at the Machine Learning Department and Computer Science Department of Carnegie Mellon University. He is also the Chief Technologist of OctoML. He received his PhD. from the Paul G. Allen School of Computer Science & Engineering at the University of Washington. He has created many major learning systems that are widely adopted: XGBoost, TVM, and MLC-LLM.


