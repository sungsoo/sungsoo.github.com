---
layout: post
title: Scaling Limits of Neural Networks
date: 2024-09-23
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source


* [Scaling Limits of Neural Networks](https://www.youtube.com/watch?v=0PJJ29jYIsg)

---



# Scaling Limits of Neural Networks


* Big Data Conference 2024, 9/7/2024
* Speaker: Boris Hanin, Princeton University
* Title: Scaling Limits of Neural Networks


## ABSTRACT 
Neural networks are often studied analytically through scaling limits: regimes in which taking some structural network parameters (e.g. depth, width, number of training datapoints, and so on) to infinity results in simplified models of learning. I will motivative and discuss recent results using several such approaches. I will emphasize both new theoretical insights into how model, training data, and optimizer impact learning and their practical implications for hyperparameter transfer.


<iframe width="600" height="400" src="https://www.youtube.com/embed/0PJJ29jYIsg?si=JOeMaGuiC1pzScYF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>