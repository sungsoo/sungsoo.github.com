---
layout: post
title: Probabilistic Circuits - Representations, Inference, Learning and Theory
date: 2022-03-30
categories: [computer science]
tags: [machine learning, graph mining]

---

### Article Source

* [Probabilistic Circuits - Representations, Inference, Learning and Theory](https://www.youtube.com/watch?v=2RAG5-L9R70)


---

# Probabilistic Circuits - Representations, Inference, Learning and Theory

* Tutorial at ECML-PKDD 2020
* Slides: [http://starai.cs.ucla.edu/slides/ECML20.pdf](http://starai.cs.ucla.edu/slides/ECML20.pdf)

## Abstract

Exact and efficient *probabilistic inference and learning* are important when we want to quickly take complex decisions in presence of uncertainty in real-world scenarios. In this tutorial, we introduce *probabilistic circuits* (PCs) as a unified computational framework to represent and learn deep probabilistic models guaranteeing tractable inference. Different from other deep neural estimators such as variational autoencoders and normalizing flows, PCs enable large classes of tractable inference with little or no compromise in terms of model expressiveness. Moreover, while showing a unified view to learn PCs from data and several real-world applications, we cast many popular tractable models in the framework of PCs while theoretically tracing the boundaries of tractable probabilistic inference.


<iframe width="600" height="400" src="https://www.youtube.com/embed/2RAG5-L9R70" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


