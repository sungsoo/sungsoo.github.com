---
layout: post
title: GPT-3 = Generation Pre-trained Transformer 3
date: 2021-05-04
categories: [computer science]
tags: [machine learning, data management]

---

### Article Source

* [GPT-3 = Generation Pre-trained Transformer 3](https://www.youtube.com/watch?v=p24JUVgDkQk&t=52s)

---

# GPT-3 = Generation Pre-trained Transformer 3

일론 머스크와 샘 알트만이 2015년 12월 11일 공동 설립한 인공지능 회사인 openAI사가 개발한 인공 일반 지능 모델이다.

## Overview

Generation Pre-trained Transformer 3(GPT-3)은 딥러닝을 이용해 인간다운 텍스트를 만들어내는 자기회귀(Autoregressive) 언어 모델이다. openAI사가 만든 GPT-*n *시리즈의 3세대 언어 예측 모델이다. GPT-3의 전체버전은 1,750억개의 매개변수를 가지고 있어, 2020년 5월 도입된 이전버전 GPT-2보다 2배 이상 크다. 2020년 7월 현재 베타 테스트 중에 있으며, 사전 훈련된 언어의 자연어 처리(NLP) 시스템의 일환이다. GPT-3 출시 전 가장 큰 언어 모델은 2020년 2월에 선보인 마이크로소프트의 튜링 NLG로 GPT-3보다 용량이 10배 적었다.
GPT-3가 수행가능한 작업으로는 각종 언어 관련 문제풀이, 랜덤 글짓기, 간단한 사칙연산, 번역, 주어진 문장에 따른 간단한 웹 코딩이 가능하다.

GPT-3에서 생성되는 본문의 퀄리티는 매우 높아 유익성과 위해성을 동시에 지닌 인간이 작성한 본문과 구별하기 어렵다. 31개 오픈AI 연구진과 엔지니어들은 GPT-3를 소개하는 2020년 5월 28일 논문 원본을 발표하면서 GPT-3의 잠재적 위험을 경고하고 위험 완화를 위한 연구를 요구했다. 호주 철학자 데이비드 찰머스는 GPT-3를 "지금까지 생산한 AI 시스템 중 가장 흥미롭고 중요한 시스템 중 하나"라고 설명했다.

2020년 10월부터 openAI는 GPT-3를 마이크로소프트 에저를 통해서 독점 공급할 것이라고 밝혔다. 지금까지 openAI가 추구해왔던 비영리, 오픈소스와 반대되는 행동이라 비판이 나오고 있는데, 한 편에선 이 정도 성능의 언어모델을 만드는데 들어간 비용을 생각해보면 어쩔 수 없었다는 의견이 나오기도 한다.

## Limitations

> The GPT-3 hype is way too much. It’s impressive (thanks for the nice compliments!) but it still has serious weaknesses and sometimes makes very silly mistakes. AI is going to change the world, but GPT-3 is just a very early glimpse. We have a lot still to figure out.

> GPT-3은 너무 과대평가되었습니다. 여러 칭찬은 감사하지만, 여전히 약점이 있고 이상한 실수를 하기도 합니다. AI가 세상을 바꿀 것이지만 GPT-3가 그 첫 발을 내딛은 것뿐이라 생각합니다. 여전히 알아낼 게 많아요. 
> - GPT-3 개발사 대표 Sam Altman


- **효율성이 너무 떨어진다.**

GPT-3은 무려 1,750억개의 매개변수를 가지고 있으며 인간이 평생 보는 정보보다 많은 데이터를 학습해야한다. 사전학습에 필요한 비용(약 50억원 이상), 시간이 너무 방대하고 활용하기도 쉽지 않다.

- **현실세계의 물리적 상식을 잘 모른다.**

GPT-3는 "치즈를 냉장고 안에 넣으면 녹을까?" 라는 질문에 "그렇다"라고 답했는데, 일반적인 사람이 당연히 알만한 물리적 지식을 잘 모른다. 이는 세상을 글로만 학습했기 때문에 눈먼 장님이 방 안에서 책을 통해 세상을 배운 것처럼, 우리가 눈을 통해 현실에서 직접 겪어봐 알 수 있는 매우 당연한 상식을 학습할 기회가 적었기 때문이다.

- **모든 분야에서 뛰어난 것은 아니다.**

아직까지는 대부분 테스크에서 사람보다 떨어진 성능을 보이며, 주어진 테스크마다 성능도 매우 차이난다. 예를 들어 두가지 이상의 복합연산 능력이 떨어지고, 테스크를 수행하기 위해 주어진 데이터가 적을수록 성능이 크게 떨어지는 경향을 보였다.

- **학습에 사용된 예제를 외운 것인지 실제 추론한 것인지 구분하기 어렵다.**

- **새로운 정보를 수용하기 어렵다. 한마디로 "기억력"이 없다.**

현재까지 모든 딥러닝 인공지능이 그러하듯, 학습된 정보를 토대로 입력값에 대해 출력값을 내보낼 수는 있지만, 사람처럼 기억력이라 부를만한 것이 없다. 물론 학습에 사용되는 정보를 입력할 수는 있지만 사람의 기억과는 다를뿐더러 그 크기도 제한되어있다. 또한 새로운 값에 대해 동기화도 잘 이루어지지 않는다.

- **GPT-3은 방대한 양의 텍스트를 통해 다음 단어를 예측하는 방식으로 학습되었다.**

GPT-3 논문에 서술되어있듯이, GPT-3은 주어진 단어에 대해 통계적으로 가장 어울리는 다음 단어를 생성하는 것뿐이며 이해하는 것은 아니라는 비판이 있다. 생각과 이해가 무엇인지는 철학의 영역이지만, 분명한건 우리 인간은 다음 단어를 예측하는 방법으로 언어를 학습하지 않았다는 점이다.


<iframe width="600" height="400" src="https://www.youtube.com/embed/p24JUVgDkQk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>