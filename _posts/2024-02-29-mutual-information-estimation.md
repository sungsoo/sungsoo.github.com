---
layout: post
title: On Mutual Information Estimation
date: 2024-02-29
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source


* [On Mutual Information Estimation](https://www.youtube.com/watch?v=QE36xa8NqPA&list=PLe0J3_6vYq7s4Xm5TEhJ_uTn9x3l9Y4Hc&index=9)

---

# On Mutual Information Estimation

* Artem Sobolev, Samsung AI Center Moscow, Research Scientist

## Abstract


Mutual Information is an important information-theoretic concept that captures an intuitive idea of the amount of information shared between two random variables. Mutual Information has been used extensively in numerous Machine Learning problems and should be of great interest for every ML researcher.
In practice, however, accurately estimating the Mutual Information is a non-trivial task. Recently, it has been shown that many general estimators fail to produce reasonable estimates unless an exponential number of samples is taken. We will discuss this result with its manifestation in several widely used estimators, and then consider new estimators that sidestep the core issue.

<iframe width="600" height="400" src="https://www.youtube.com/embed/QE36xa8NqPA?si=xHKyJoiKTaos8522" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>