---
layout: post
title: Approximate Nearest Neighbor Search in High Dimensions
date: 2020-10-25
categories: [computer science]
tags: [machine learning]

---

### Article Source
* [Approximate Nearest Neighbor Search in High Dimensions](https://www.youtube.com/watch?v=cn15P8vgB1A)

----

# Approximate Nearest Neighbor Search in High Dimensions

* Mathematical Aspects of Computer Science
* Invited Lecture 14.7
* *Approximate nearest neighbor search in high dimensions*
* Speaker: Piotr Indyk
* ICML 2018
 
## Abstract
The *nearest neighbor* problem is defined as follows: Given a set *P* of *n* points in some metric space (*X,ùñ£*), build a data structure that, given any point *q*, returns a point in *P* that is closest to *q* (its "nearest neighbor" in *P*). The data structure stores additional information about the set *P*, which is then used to find the nearest neighbor without computing all distances between *q* and *P*. The problem has a wide range of applications in machine learning, computer vision, databases and other fields. 
 To reduce the time needed to find nearest neighbors and the amount of memory used by the data structure, one can formulate the '*approximate*' nearest neighbor problem, where the the goal is to return any point *p'* ‚àà *P* such that the distance from *q* to *p'* is at most *c ‚ãÖ min_{p ‚àà P} ùñ£(q,p)*, for some *c ‚â• 1*. Over the last two decades many efficient solutions to this problem were developed. In this article we survey these developments, as well as their connections to questions in geometric functional analysis and combinatorial geometry. 

<iframe width="600" height="400" src="https://www.youtube.com/embed/cn15P8vgB1A" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
