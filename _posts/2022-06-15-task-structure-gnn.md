---
layout: post
title: Task structure and generalization in graph neural networks
date: 2022-06-15
categories: [computer science]
tags: [machine learning, graph mining]

---

### Article Source

* [Task structure and generalization in graph neural networks](https://www.youtube.com/watch?v=N67CAjI3Axw)


---

# Task structure and generalization in graph neural networks


* Deep Learning and Combinatorial Optimization 2021
* "Task structure and generalization in graph neural networks"
* Stefanie Jegelka - Massachusetts Institute of Technology
* Institute for Pure and Applied Mathematics, UCLA
* February 25, 2021
* For more information: [https://www.ipam.ucla.edu/dlc2021​](https://www.ipam.ucla.edu/dlc2021​)

## Abstract
Graph Neural Networks (GNNs) have become a popular tool for learning certain algorithmic tasks. But, their generalization properties are less well understood. Empirically, we observe an interplay between the structure of the task — or target algorithm — and the inductive biases of the architecture: although many networks may be able to represent a task, some architectures learn it better than others. In this talk, I will show an approach to formalize this relationship, and empirical and theoretical implications for generalization within and out of the training distribution. This talk is based on joint work with Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du and Ken-ichi Kawarabayashi.

<iframe width="600" height="400" src="https://www.youtube.com/embed/N67CAjI3Axw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>