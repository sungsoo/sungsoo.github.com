---
layout: post
title: Data Drift, Quality, Bias and Explainability 
date: 2024-07-24
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source


* [Intro to ML Monitoring; Data Drift, Quality, Bias and Explainability](https://www.youtube.com/watch?v=nx6S3OuIyGQ)

---



# Data Drift, Quality, Bias and Explainability 

## Abstract

Workshop Links:

- Free WhyLabs Signup: [https://whylabs.ai/free](https://whylabs.ai/free)

- Notebook: [https://bit.ly/ml-monitor-colab](https://bit.ly/ml-monitor-colab)

- whylogs github (give us a star!) [https://github.com/whylabs/whylogs/](https://github.com/whylabs/whylogs/)

- Join The AI Slack group: [https://bit.ly/r2ai-slack](https://bit.ly/r2ai-slack)



LLM monitoring: [https://github.com/whylabs/langkit](https://github.com/whylabs/langkit)

If you want to build reliable pipelines, trustworthy data, and responsible AI applications, you need to validate and monitor your data & ML models!



In this workshop weâ€™ll cover how to ensure model reliability and performance to implement your own AI observability solution from start to finish.


<iframe width="600" height="400" src="https://www.youtube.com/embed/nx6S3OuIyGQ?si=_-biLH2sPeiq6uUx" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>