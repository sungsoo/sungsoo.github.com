---
layout: post
title: Audio-visual self-supervised baby learning
date: 2024-06-20
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source


* [Audio-visual self-supervised baby learning](https://www.youtube.com/watch?v=HKa17CupqhE)

---


# Audio-visual self-supervised baby learning

* Andrew Zisserman (Oxford University)
* Understanding Lower-Level Intelligence from AI, Psychology, and Neuroscience Perspectives

## Abstract

Lesson 1 from the classic paper "The Development of Embodied Cognition: Six Lessons from Babies" is `Be Multimodal'. This talks explores how recent work in the computer vision literature on audio-visual self-supervised learning addresses this challenge. The aim is to learn audio and visual representations and capabilities directly from the audio-visual data stream of a video (without providing any manual supervision of the data) - much as an infant could learn from the correspondence and synchronization between what they see and hear. It is shown that a neural network that simply learns to synchronize audio and visual streams is able to localize the faces that are speaking (active speaker detection) and objects that sound.


<iframe width="600" height="400" src="https://www.youtube.com/embed/HKa17CupqhE?si=hyjdj7421fD01jzs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>