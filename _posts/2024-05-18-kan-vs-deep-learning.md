---
layout: post
title: Kolmogorov-Arnold Networks VS Regular Deep Learning 
date: 2024-05-18
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source


* [Kolmogorov-Arnold Networks VS Regular Deep Learning](https://www.youtube.com/watch?v=XjPKDulQHRY)

---

# Kolmogorov-Arnold Networks VS Regular Deep Learning 

* Paper: [https://arxiv.org/abs/2404.19756](https://arxiv.org/abs/2404.19756)
* Code: [https://github.com/KindXiaoming/pykan](https://github.com/KindXiaoming/pykan)

## Abstract

MASSIVE idea proposed in this paper. 

Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs) for approximating nonlinear functions ðŸ¤¯.

ðŸ“Œ Moving activation functions from nodes (neurons) to edges (weights)!

ðŸ“Œ MLPs place activation functions on neurons, but can we instead place (learnable) activation functions on weights? Yes, we KAN! 

<iframe width="600" height="400" src="https://www.youtube.com/embed/XjPKDulQHRY?si=tdZhFheUVHo8gouW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>