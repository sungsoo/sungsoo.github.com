---
layout: post
title:  AGI Alignment Techniques
date: 2024-12-23
categories: [artificial intelligence]
tags: [machine learning]

---

# AGI Alignment Techniques

# 현재 정렬 기법

현재 AGI 정렬 기술은 달성하고자 하는 목표에 따라 다양한 방식으로 구분될 수 있습니다. 가장 강력하고 일반적으로 사용되는 방법은 인간의 피드백을 활용하는 방법입니다. 이는 인간 감독자가 초거대 언어 모델(LLM)을 학습시키고, 지속적으로 개선하는 방식입니다. 

이러한 인간 피드백 기반 학습 방법은 온라인 인간 감독과 오프라인 인간 감독으로 나눌 수 있습니다. 온라인 인간 감독은 모델 학습 중에 실시간으로 인간의 피드백을 받아 모델을 개선하는 방식이고, 오프라인 인간 감독은 학습 전에 미리 수집된 데이터를 기반으로 모델을 학습시키는 방식입니다. 두 방식 모두 스케일 가능한 감독 시스템의 구성 요소가 될 수 있습니다.

## 온라인 인간 피드백

* **보상 모델:** 보상 모델은 정렬 과정에서 가장 중요한 부분입니다. Sparrow는 적대적 프로빙과 언어 기반 규칙을 RLHF 보상 모델에 통합했습니다. Bai 등은 순수 강화 학습을 통해 LLM 학습에 대한 인간 수준의 온라인 감독을 제공하고, 유용성과 무해성 사이의 트레이드오프를 자세히 탐구했습니다.
* **최적화:** 온라인 또는 오프라인 데이터를 어떻게 통합할지에 대한 연구가 활발합니다. Cheng 등은 최대-최소 최적화를 통해 보상 모델과 정책 모델을 동시에 최적화했습니다.
* **데이터:** SENSI는 언어 모델을 사용하여 보상 할당(비평자 역할)과 생성 제어(행위자 역할)를 통해 인간의 가치 판단을 언어 생성의 각 단계에 내재시키는 것을 시도했습니다. Baheti 등은 다양한 가중치를 부여하여 기존 학습 데이터를 보강하고, 언어 모델에 대한 기여도를 최대화했습니다.
* **자기 개선:** 강력한 AI 모델은 외부 감독 없이 스스로 학습하고 개선해야 합니다. f-DPG는 KL 발산 대신 임의의 목표 분포를 근사화할 수 있도록 RLHF를 일반화했습니다. Zhu 등은 RLHF를 최대 엔트로피 IRL과 연결하여 통합적인 프레임워크를 제안했습니다.

## 기타 RL 기반 방법

RLHF 외에도 다른 RL 기반 방법들이 연구되고 있습니다. Second Thoughts는 텍스트 편집 과정을 통해 학습 데이터를 보강하고 RL 알고리즘을 활용하여 LLM을 학습시킵니다. RLAIF는 AI 생성 데이터를 강화 학습에 활용하여 더욱 뛰어난 생성 모델로부터 지식 증류를 가능하게 합니다. Kim 등은 인간이 주석한 선호 데이터 대신 자동으로 학습 데이터를 생성하는 RLSF를 제안했습니다.

## 결론

현재 AGI 정렬 연구는 빠르게 발전하고 있으며, 다양한 기술과 접근법이 제시되고 있습니다. 특히 인간 피드백을 활용한 강화 학습 기반 방법들이 주목받고 있으며, 보상 모델, 최적화, 데이터, 자기 개선 등 다양한 측면에서 연구가 진행되고 있습니다. 앞으로는 더욱 복잡하고 정교한 AI 시스템을 안전하게 개발하고 배포하기 위해 이러한 연구가 더욱 활발하게 이루어질 것으로 예상됩니다.

**핵심 키워드:** AGI 정렬, 인간 피드백, 강화 학습, RLHF, 보상 모델, 최적화, 데이터, 자기 개선

# AGI 정렬에서 오프라인 인간 감독 방식을 활용한 RL 기반 방법론

온라인 인간 감독 방식과 달리, 오프라인 인간 감독 방식은 모델 학습 전에 미리 수집된 데이터를 기반으로 모델을 학습시키는 방법입니다. 이는 보상 모델 학습의 불안정성과 시스템 오류 발생 가능성을 줄일 수 있다는 장점이 있습니다. 

텍스트는 오프라인 인간 감독 방식을 크게 두 가지로 나누어 설명합니다.

## 1. 텍스트 기반 피드백 신호

* **CoH:** 인간의 학습 과정에서 영감을 얻어, 모델 출력에 대한 피드백을 통해 모델을 지속적으로 조정하는 방식입니다. 이전 추론 단계에서 얻은 피드백을 바탕으로 다음 출력을 예측하고, 이를 통해 모델을 미세 조정합니다.
* **RAFT:** 보상 모델을 사용하여 모델 출력을 인간의 선호도와 일치시키는 방식입니다. 하지만 오프라인 방식으로 진행됩니다.
* **LIMA:** LLM이 사전 학습 과정에서 대부분의 지식을 습득한다는 가정 하에, 최소한의 지시-조정 데이터만으로도 원하는 출력을 생성하도록 합니다.
* **ILF:** 베이지안 추론과 유사하게, 언어 피드백을 기반으로 인간의 선호도를 모델링하는 3단계 과정을 제시합니다.
* **Stable Alignment:** 샌드박스 시뮬레이터를 사용하여 다중 에이전트 상호 작용을 통해 모델을 직접적으로 선호도 데이터로 학습시키는 방식입니다.
* **SteerLM:** 사용자가 명시적으로 정의한 다차원 속성 집합에 따라 응답을 조절할 수 있도록 모델을 학습시킵니다.
* **CLP:** 다중 작업 학습과 매개변수 효율적 미세 조정 기법을 기반으로, 추론 시 충돌하는 목표 간의 트레이드오프를 효과적으로 처리하는 모델을 학습시킵니다.

## 2. 순위 기반 피드백 신호

* **CRINGE:** 모델이 피해야 할 부정적인 예시를 학습시켜 모델을 개선하는 방식입니다.
* **Xu et al.**: 유해한 콘텐츠를 생성하는 모델을 학습시켜 기존 모델을 미세 조정하는 방식입니다.
* **Schick et al.**: 유해한 텍스트 유형에 해당하는 텍스트를 식별하고 생성하는 방법론을 제시합니다.
* **SLiC:** 다양한 손실 함수를 사용하여 참조 시퀀스와의 정렬을 통해 출력 시퀀스의 확률을 개선합니다.
* **RRHF:** 순위를 통해 자동으로 정렬 감독 신호를 생성하는 방식입니다.
* **DPO:** 인간의 선호도와 일치하도록 모델을 직접적으로 최적화하는 방식입니다.
* **IPO:** DPO를 확장하여 정규화 항을 도입하여 학습 과정을 안정화시킵니다.
* **PRO:** 순위 데이터를 사용하여 모델을 최적화하는 방식입니다.
* **KTO:** 개별 예시를 "좋음" 또는 "나쁨"으로 라벨링하고 손실 함수를 정의하는 방식입니다.
* **BoNBON Alignment:** Best-of-N 샘플링 분포를 모방하도록 LLM을 미세 조정하는 방식입니다.
* **BOND:** BoNBON의 계산 비용을 줄이면서 유사한 성능을 내는 새로운 RLHF 알고리즘입니다.
* **vBoN:** BoN 알고리즘에 의해 유도된 확률 분포를 근사화하는 방식입니다.

**핵심 내용**

* 오프라인 인간 감독 방식은 텍스트 기반 피드백 신호와 순위 기반 피드백 신호로 나눌 수 있습니다.
* 각 방식은 모델 학습에 활용하는 데이터와 최적화 방식에 차이가 있습니다.
* 다양한 연구를 통해 각 방식의 장단점과 개선 방안이 제시되고 있습니다.

# 스케일러블 감독 방법: 인간의 능력을 넘어서는 AI 정렬을 위한 접근법

**스케일러블 감독**이란 인간이 초지능 AI 시스템을 감독하고 제어하는 방법을 의미합니다. 즉, 인간보다 훨씬 뛰어난 능력을 가진 AI 시스템을 인간이 이해하고 관리할 수 있도록 하는 기술입니다. 이는 점진적으로 진화하는 신호를 활용하여 상대적으로 약한 감독자(예: 인간)가 복잡한 작업이나 초인적인 모델을 감독할 수 있게 함으로써, 인간의 능력을 넘어서는 문제를 해결하는 데 기여합니다.

## 작업 분해를 통한 스케일러블 감독

복잡한 작업을 더 작고 독립적인 하위 작업으로 분해하여 각각을 동시에 처리하거나 순차적으로 처리하는 방식입니다. 이를 통해 전체 작업의 복잡도를 낮추고, 각 하위 작업에 대한 감독을 용이하게 합니다.

* **요인화된 인지(Factored Cognition):** 복잡한 작업을 작은 독립적인 하위 작업으로 분해하여 동시에 처리합니다.
* **프로세스 감독:** 복잡한 작업을 순차적인 하위 작업으로 분해하고 각 단계에 대한 감독 신호를 설정합니다.
* **샌드위치:** 복잡한 작업을 도메인 전문가에게 위임하여 해결합니다.
* **IDA:** 반복적인 증류와 증폭 과정을 통해 모델의 능력을 향상시킵니다.
* **RRM:** 보상 모델링을 통해 인간과 AI 에이전트 간의 협력을 강화하고 학습을 개선합니다.

## 인간이 작성한 원칙을 통한 스케일러블 감독

AI 시스템이 따라야 할 일반적인 원칙을 인간이 제공하고, 이를 기반으로 AI 시스템이 학습 데이터를 생성하도록 하는 방식입니다. 

* **헌법 AI:** AI 시스템이 따라야 할 일반적인 원칙을 제공하고, 이를 기반으로 AI 시스템이 학습 데이터를 생성합니다.
* **Dromedary:** RL 없이도 인간이 작성한 원칙을 기반으로 자기 지시 및 자기 정렬 방법을 사용하여 초인간적인 AI 시스템을 개발합니다.

## 모델 상호 작용을 통한 스케일러블 감독

모델 간의 상호 작용을 통해 문제를 해결하고, 이를 통해 스케일러블 감독을 달성하는 방식입니다.

* **토론 패러다임:** 에이전트가 질문에 대한 답변을 제시하고, 다른 에이전트와 구조화된 토론을 통해 자신의 주장을 정당화하고 비판합니다.
* **시장 메이킹:** 시장 모델과 적대자 모델이 상호 작용하며 질문에 대한 답변을 예측하고 생성하며, 적대자는 시장 모델의 예측을 변경하기 위해 주장을 제시합니다.

## 결론

스케일러블 감독은 인간이 초지능 AI 시스템을 안전하게 관리하고 제어하기 위한 필수적인 기술입니다. 위에서 설명한 다양한 방법론들은 각각의 장단점을 가지고 있으며, 앞으로 더욱 발전된 기술들이 개발될 것으로 예상됩니다.

**핵심 키워드:** 스케일러블 감독, 초지능 AI, 인간-AI 협력, 작업 분해, 인간이 작성한 원칙, 모델 상호 작용

