---
layout: post
title: Deploy Model to KServe
date: 2022-07-14
categories: [computer science]
tags: [machine learning, graph mining]

---

### Article Source

* [Deploy Model to KServe](https://www.youtube.com/watch?v=f46xzB4L6I4)


---

# Deploy Model to KServe


## Abstract

In this tutorial, we take a model container built using the Chassis.ml service and walk through the steps required to deploy the model to KServe. KServe is an open-source model inferencing platform that runs on top of Kubernetes.


<iframe width="600" height="400" src="https://www.youtube.com/embed/f46xzB4L6I4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Kubeflow Tutorial | Model Serving

In this video, I walk you through a simple model engineering process using Kubeflow Fairing (Note: nowadays, there is a better way to serve models, with Kubeflow Serving)

<iframe width="560" height="315" src="https://www.youtube.com/embed/hRmmzItkPkA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>