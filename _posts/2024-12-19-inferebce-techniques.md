---
layout: post
title:  Inference Techniques
date: 2024-12-19
categories: [artificial intelligence]
tags: [machine learning]

---

# Inference Techniques

# 추론 기법

AGI 추론 시스템은 사용자 응답성, 가용성 및 효율성을 보장해야 하며, 이는 훈련 단계에서 대규모 모델의 잠재력을 최대한 활용하고 사용자와 시스템의 상호 작용 방식을 혁신하는 데 도움이 됩니다. 따라서 본 절에서는 자기 회귀적 디코딩 가속화, 요청 스케줄링 균형 조정, 클러스터 내에서 다양한 기능을 가진 대규모 모델을 서비스하는 등 향후 시스템 개발에 영감을 줄 수 있는 다양한 기법들을 개괄적으로 살펴봅니다.

## 디코딩 알고리즘

본 논문에서는 정확도를 훼손하지 않으면서 디코딩 성능을 최대화하는 정확한 디코딩 가속에 주목합니다. Miao 등(2023)은 샘플링 전략, 비자기 회귀적 디코딩, 반자기 회귀적 디코딩, 블록 병렬 디코딩 등 다양한 근사 방법에 대한 포괄적인 검토를 제공합니다.

많은 연구들이 여러 토큰을 한 번에 생성할 가능성을 높여 병렬 계산을 활용하는 투기적 디코딩 아이디어를 탐구합니다. 일반적으로 투기적 디코딩 프로세스는 여러 단계를 예측하는 효율적인 드래프트 모델로 시작하며, 생성된 제안은 목표 모델을 통해 검증됩니다. 하지만 드래프트 모델을 가볍게 유지하면서도 유용한 추측을 생성하는 방법, 아키텍처 변경과 미세 조정을 최소화하여 빠르게 적응하는 방법, 드래프트 모델을 더 효과적으로 배포하는 방법 등 해결해야 할 과제들이 많습니다. 

가장 간단하면서도 효과적인 변형은 프롬프트 조회 디코딩(Saxena, 2023)으로, 드래프트 모델 대신 기존 데이터베이스에서 접두사 문자열 매칭을 통해 후보 토큰을 생성하는 것입니다. 이 모델 독립적인 접근 방식은 미세 조정이나 모델 변경 없이 매우 빠르게 디코딩할 수 있지만, 성능은 문자열 풀의 품질과 다양성에 크게 의존합니다. 후보를 더 빠르게 검증하기 위해 SpecInfer(Miao et al., 2024)는 드래프트 모델의 출력을 각 노드가 후보 토큰인 토큰 트리로 구성하여, 각 노드의 정확성을 기본 모델이 병렬로 효율적으로 검사할 수 있도록 합니다. 유사한 아이디어로 Medusa(Cai et al., 2024a)는 특수 마스크 패턴을 통해 모든 토큰을 동시에 검사하는 트리 어텐션 메커니즘을 도입합니다. Self-speculative decoding(Zhang et al., 2023k)은 드래프트 모델을 완전히 버리고 중간 레이어의 일부를 선택적으로 건너뛰어 후보 시퀀스를 생성합니다.

## 하드웨어 인지 알고리즘

하드웨어 인지 알고리즘은 디코딩 단계에서 특히 효과적이고 매력적입니다. Flash-Decoding은 시퀀스 차원을 따라 분할하고 이 블록들을 Flash-Attention으로 병렬 처리하여 KV 캐시와 통계를 사용하여 결과를 집계합니다. FlashDecoding++(Hong et al., 2023a)는 Flash-Decoding의 한계를 해결하고 시스템 수준의 최적화를 적용하여 디코딩 절차를 가속화합니다. 비동기 소프트맥스, 이중 버퍼링을 사용한 최적화된 플랫 GEMM 연산, 하드웨어 자원 적응을 위한 휴리스틱 기반 데이터 흐름 등을 도입하여 HuggingFace 대비 4배 이상 속도 향상을 달성했습니다. 

## 요약

본 절에서는 AGI 추론 시스템의 성능을 향상시키기 위한 다양한 디코딩 가속 기법들을 소개했습니다. 특히 투기적 디코딩, 하드웨어 인지 알고리즘 등을 중심으로 자세히 살펴보았습니다. 이러한 기법들은 대규모 모델의 실제적인 활용을 위한 중요한 토대가 될 것입니다. 

**핵심 키워드:** 자기 회귀적 디코딩, 투기적 디코딩, 하드웨어 인지 알고리즘, Flash-Decoding, SpecInfer, Medusa


# 요청 스케줄링: LLM 추론의 핵심 과제

LLM 추론 시스템의 효율적인 운영을 위해서는 요청 스케줄링이 필수적입니다. 기존의 구조화된 입력을 다루는 머신 러닝 시스템과 달리, LLM은 1) 사용자 정보, 과거 KV 캐시, 모델 어댑터 등 다양한 맥락 정보를 사전에 가져와야 하고, 2) 가변적인 시퀀스 길이를 가진 예제들을 처리해야 하며, 3) 시간당 첫 토큰 생성 시간(TTFT), 작업 완료 시간(JCT), 배치 토큰 처리량, 추론 지연 시간 등 다양한 지표를 고려해야 하는 복잡한 특징을 가지고 있습니다.

## 기존 연구 및 해결 방안

* **Orca:** LLM 추론의 자기 회귀적인 특성을 고려하여 반복적인 스케줄링 메커니즘을 제안했습니다. 이를 선택적 배치 기법과 결합하여 하드웨어 활용도를 높이고, 기존 추론 엔진보다 높은 처리량과 낮은 지연 시간을 달성했습니다.
* **vLLM, TensorRT-LLM:** 연속적인 배치, 비행 중 배치 등 동적 배치 기법을 통해 LLM 추론의 효율성을 높였습니다.
* **FastServe:** LLM 추론의 자기 회귀적 패턴을 활용하여 각 출력 토큰 단위로 선점이 가능하도록 하였습니다. 새로운 다단계 피드백 큐 스케줄러를 통해 JCT를 최적화하고, 입력 시퀀스 길이 정보를 활용하여 효율성을 높였습니다.
* **S³:** 배치 내 각 예제의 잠재적인 응답 길이를 예측하여 메모리 제약 조건 하에서 더 많은 예제를 처리할 수 있도록 했습니다.
* **DeepSpeed-FastGen:** LLM 추론의 특성을 고려하여 토큰 구성 전략을 제안했습니다. 긴 프롬프트는 여러 전방향 반복을 통해 작은 청크로 분할하고, 짧은 프롬프트는 다른 요청과 정렬되도록 구성합니다. 이를 통해 시스템은 더 나은 효율성과 응답성을 제공하며, 요청 간 변동성을 줄입니다.

## 요약 및 시사점

LLM 추론 시스템의 효율성을 높이기 위해 다양한 요청 스케줄링 기법들이 제안되고 있습니다. 이러한 기법들은 LLM의 자기 회귀적 특성, 가변적인 시퀀스 길이, 하드웨어 제약 조건 등을 고려하여 최적의 성능을 추구합니다. 앞으로는 더욱 정교한 스케줄링 알고리즘과 하드웨어 최적화 기술을 통해 LLM 추론 시스템의 성능이 더욱 향상될 것으로 기대됩니다.

## 추가 연구 방향

* **다양한 LLM 아키텍처에 대한 적용:** Transformer 외에도 다양한 아키텍처에 대한 스케줄링 기법 연구가 필요합니다.
* **실시간 상호 작용 시스템을 위한 스케줄링:** 사용자와의 실시간 상호 작용이 필요한 시스템에 적합한 스케줄링 기법 연구가 필요합니다.
* **에너지 효율적인 스케줄링:** 에너지 효율성을 고려한 스케줄링 기법 연구가 필요합니다.
* **분산 환경에서의 스케줄링:** 분산 환경에서의 LLM 추론을 위한 스케줄링 기법 연구가 필요합니다.

# 멀티 모델 서빙: 다양한 모델을 효율적으로 서비스하기 위한 접근

## 개요

이 글은 **다양한 모델을 하나의 시스템에서 효율적으로 서비스하는 방법**에 대한 최신 연구 동향을 소개합니다. 특히, **PEFT(Parameter-Efficient Fine-Tuning)** 기술을 활용하여 기본 모델에 다양한 어댑터를 적용하는 방식이 주목받고 있습니다. 이는 모델의 유연성과 효율성을 동시에 확보할 수 있는 매력적인 접근 방식입니다.

## 멀티 모델 서빙의 중요성

* **다양한 애플리케이션:** LLM 에이전트, 페르소나 챗봇, 프라이버시 보호 어시스턴트 등 다양한 애플리케이션에서 개별 작업에 특화된 모델을 필요로 합니다.
* **PEFT 기술의 발전:** PEFT 기술은 경량화된 모델을 빠르게 생성하고 유지 관리할 수 있어 멀티 모델 서빙에 적합합니다.

## 멀티 모델 서빙의 주요 과제 및 해결 방안

* **적절한 어댑터 선택:** 각 요청에 맞는 최적의 어댑터를 동적으로 선택하는 것이 중요합니다.
* **효율적인 계산:** 다양한 어댑터를 효율적으로 처리하기 위한 최적화된 알고리즘이 필요합니다.

**Punica, S-LORA, LORAX** 등의 연구는 이러한 과제를 해결하기 위한 다양한 접근 방식을 제시합니다.

* **Punica:** 하나의 사전 학습된 모델에 다양한 LoRA 헤드를 효율적으로 계산하는 CUDA 커널을 개발하여 처리량을 12배 이상 향상시켰습니다.
* **S-LORA:** 통합 메모리 풀을 사용하여 어댑터를 동적으로 관리하고, LoRA 계산을 병렬화하여 효율성을 높였습니다.
* **LORAX:** 어댑터 교환 스케줄링을 통해 GPU와 CPU 메모리 간 어댑터를 비동기적으로 전송하고, 요청 배치를 최적화하여 처리량을 향상시켰습니다.

## 멀티 모델 서빙의 미래

위와 같은 연구를 통해 단일 GPU에서 수천 개의 LoRA 헤드를 서비스하는 것이 가능해졌습니다. 이는 모델 협업, 작업 일반화, 모델 병합 등 다양한 가능성을 열어줍니다.

## 결론

멀티 모델 서빙은 다양한 모델을 효율적으로 서비스하여 AI 애플리케이션의 범위를 확장하는 데 중요한 역할을 합니다. PEFT 기술과 최적화된 알고리즘을 통해 멀티 모델 서빙의 성능을 더욱 향상시킬 수 있을 것입니다.

## 추가 연구 방향

* **다양한 모델 아키텍처 지원:** Transformer 외에도 다양한 아키텍처에 대한 멀티 모델 서빙 연구가 필요합니다.
* **대규모 멀티 모델 시스템:** 수천 개 이상의 모델을 효율적으로 관리하는 시스템에 대한 연구가 필요합니다.
* **모델 선택 및 최적화 자동화:** 최적의 모델을 자동으로 선택하고 최적화하는 기술 개발이 필요합니다.
* **에너지 효율적인 멀티 모델 서빙:** 에너지 효율성을 고려한 멀티 모델 서빙 시스템 연구가 필요합니다.

## 핵심 키워드

* 멀티 모델 서빙
* PEFT
* LoRA
* Punica
* S-LORA
* LORAX
* CUDA
* 어댑터
* 모델 협업
* 작업 일반화

