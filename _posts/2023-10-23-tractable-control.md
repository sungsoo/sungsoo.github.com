---
layout: post
title: Tractable Control for Autoregressive Language Generation 
date: 2023-10-23
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source

* [Tractable Control for Autoregressive Language Generation](https://www.youtube.com/watch?v=nexPWTVT_p0)

---

# Tractable Control for Autoregressive Language Generation

* Honghua Zhang (UCLA)
* [https://simons.berkeley.edu/talks/honghua-zhang-ucla-2023-10-20](https://simons.berkeley.edu/talks/honghua-zhang-ucla-2023-10-20)
* Probabilistic Circuits and Logic

## Abstract

Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution ${\Pr}(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models (TPMs) to impose lexical constraints in autoregressive text generation models, which we refer to as \textbf{GeLaTo} (Generating Language with Tractable Constraints). To demonstrate the effectiveness of this framework, we use distilled hidden Markov models, where we can efficiently compute ${\Pr}(\text{text} | \alpha)$, to guide autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on challenging benchmarks for constrained text generation (e.g., CommonGen), beating various strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive TPMs.


<iframe width="600" height="400" src="https://www.youtube.com/embed/nexPWTVT_p0?si=XR7n0-Xd1XHtVTwb" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
