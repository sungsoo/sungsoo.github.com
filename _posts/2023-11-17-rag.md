---
layout: post
title: Retrieval-Augmented Language Model Pre-Training 
date: 2023-11-17
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source

* [Retrieval-Augmented Language Model Pre-Training](https://www.youtube.com/watch?v=JaVBG7tFAU8&list=PLXtAHOcKKDTltoyBYhHECCny4r0aUXqON)

---

# Retrieval-Augmented Language Model Pre-Training

* [REALM: Retrieval-Augmented Language Model Pre-Training](https://proceedings.mlr.press/v119/guu20a/guu20a.pdf)
* [REALM: Integrating Retrieval into Language Representation Models](https://blog.research.google/2020/08/realm-integrating-retrieval-into.html)
* [google-research/language](https://github.com/google-research/language/tree/master/language/realm)

## Abstract

REALM is just the BERT based language model (LM) augmented with a retriever.
At pretrain time, MLM task is used. Pretraining is unsupervised where salient spans (entities, dates) are masked. 
Finetuning is done for Open QA task: predict (start, end) spans given question and retrieved docs z.
Retriever is supported by knowledge corpus and MIPS. The paper showed great results for OpenQA task. It formed a basis for many future studies: RAG, RETRO, ATLAS.

In this I will talk about REALM training details and also its evaluation on open question answering datasets.

<iframe width="600" height="400" src="https://www.youtube.com/embed/JaVBG7tFAU8?si=otXphgzfpUtxzBWW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>