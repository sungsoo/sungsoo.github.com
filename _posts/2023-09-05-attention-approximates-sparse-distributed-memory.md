---
layout: post
title: Attention Approximates Sparse Distributed Memory
date: 2023-09-05
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source

* [Attention Approximates Sparse Distributed Memory](https://www.youtube.com/watch?v=L4DC7e6g2iI)

---

# Attention Approximates Sparse Distributed Memory

* Trenton Bricken, PhD student at Harvard
* Will Dorrell, PhD student at University College London's Gatsby Unit

## Abstract

In this speaker series, we examine the details of how transformers work, and dive deep into the different kinds of transformers and how they're applied in different fields. We do this by inviting people at the forefront of transformers research across different domains for guest lectures. 

<iframe width="600" height="400" src="https://www.youtube.com/embed/L4DC7e6g2iI?si=HjPnh7r1yG6YFriE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
