---
layout: post
title: MLP vs KAN, Math, B-Splines, Universal Approximation Theorem 
date: 2024-05-19
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source


* [Kolmogorov-Arnold Networks; MLP vs KAN, Math, B-Splines, Universal Approximation Theorem](https://www.youtube.com/watch?v=-PFIkkwWdnM)

---

# Kolmogorov-Arnold Networks VS Regular Deep Learning 

In this video, I will be explaining Kolmogorov-Arnold Networks, a new type of network that was presented in the paper "KAN: Kolmogorov-Arnold Networks" by Liu et al.
I will start the video by reviewing Multilayer Perceptrons, to show how the typical Linear layer works in a neural network. I will then introduce the concept of data fitting, which is necessary to understand BÃ©zier Curves and then B-Splines.
Before introducing Kolmogorov-Arnold Networks, I will also explain what is the Universal Approximation Theorem for Neural Networks and its equivalent for Kolmogorov-Arnold Networks called Kolmogorov-Arnold Representation Theorem.
In the final part of the video, I will explain the structure of this new type of network, by deriving its structure step by step from the formula of the Kolmogorov-Arnold Representation Theorem, while comparing it with Multilayer Perceptrons at the same time.
We will also explore some properties of this type of network, for example the easy interpretability and the possibility to perform continual learning.

* Paper: [https://arxiv.org/abs/2404.19756](https://arxiv.org/abs/2404.19756)
* Slides PDF: [https://github.com/hkproj/kan-notes](https://github.com/hkproj/kan-notes)

<iframe width="600" height="400" src="https://www.youtube.com/embed/-PFIkkwWdnM?si=dArKnzuajigB1SKY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
