---
layout: post
title: Multimodal Large Language Model Tutorial
date: 2024-07-17
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source


* [Multimodal Large Language Model Tutorial](https://www.youtube.com/watch?v=pHBT3zXxQX8)

---


# Multimodal Large Language Model Tutorial

* [Web site](https://mllm2024.github.io/CVPR2024/)


## Abstract 

 Artificial intelligence (AI) encompasses knowledge acquisition and real-world grounding across various modalities. As a multidisciplinary research field, multimodal large language models (MLLMs) have recently garnered growing interest in both academia and industry, showing an unprecedented trend to achieve human-level AI via MLLMs. These large models offer an effective vehicle for understanding, reasoning, and planning by integrating and modeling diverse information modalities, including language, visual, auditory, and sensory data. This tutorial aims to deliver a comprehensive review of cutting-edge research in MLLMs, focusing on four key areas: MLLM architecture design, instructional learning&hallucination, multimodal reasoning of MLLMs and efficient learning in MLLMs. We will explore technical advancements, synthesize key challenges, and discuss potential avenues for future research.


<iframe width="600" height="400" src="https://www.youtube.com/embed/pHBT3zXxQX8?si=Zch_8nJebZSY4ylp" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


## Literature

#### Section I: LLMs and MLLMs

1.  OpenAI, 2023, [**Introducing
    ChatGPT**](https://openai.com/blog/chatgpt)
2.  OpenAI, 2023, [**GPT-4 Technical
    Report**](https://arxiv.org/abs/2303.08774)
3.  Alayrac, et al., 2022, [**Flamingo: a Visual Language Model for
    Few-Shot Learning**](https://arxiv.org/abs/2204.14198)
4.  Li, et al., 2023, [**BLIP-2: Bootstrapping Language-Image
    Pre-training with Frozen Image Encoders and Large Language
    Models**](https://arxiv.org/abs/2301.12597)
5.  Zhu, et al., 2023, [**MiniGPT-4: Enhancing Vision-Language
    Understanding with Advanced Large Language
    Models**](https://arxiv.org/abs/2304.10592)
6.  Wu, et al., 2023, [**Visual ChatGPT: Talking, Drawing and Editing
    with Visual Foundation Models**](https://arxiv.org/abs/2303.04671)
7.  Shen, et al., 2023, [**HuggingGPT: Solving AI Tasks with ChatGPT and
    its Friends in Hugging Face**](https://arxiv.org/abs/2303.17580)
8.  Tang, et al., 2023, [**Any-to-Any Generation via Composable
    Diffusion**](https://arxiv.org/abs/2305.11846)
9.  Girdhar, et al., 2023, [**ImageBind: One Embedding Space To Bind
    Them All**](https://arxiv.org/abs/2305.05665)
10. Wu, et al., 2023, [**NExT-GPT: Any-to-Any Multimodal
    LLM**](https://arxiv.org/abs/2309.05519)
11. Moon, et al., 2023, [**AnyMAL: An Efficient and Scalable
    Any-Modality Augmented Language
    Model**](https://arxiv.org/abs/2309.16058)
12. Hu, et al., 2023, [**Large Multilingual Models Pivot Zero-Shot
    Multimodal Learning across
    Languages**](https://arxiv.org/abs/2308.12038)
13. Bai, et al., 2023, [**Qwen-VL: A Versatile Vision-Language Model for
    Understanding, Localization, Text Reading, and
    Beyond**](https://arxiv.org/abs/2308.12966)
14. Wang, et al., 2023, [**CogVLM: Visual Expert for Pretrained Language
    Models**](https://arxiv.org/abs/2311.03079)
15. Peng, et al., 2023, [**Kosmos-2: Grounding Multimodal Large Language
    Models to the World**](https://arxiv.org/abs/2306.14824)
16. Dong, et al., 2023, [**InternLM-XComposer2: Mastering Free-form
    Text-Image Composition and Comprehension in Vision-Language Large
    Model**](https://arxiv.org/abs/2401.16420)
17. Zhu, et al., 2023, [**LanguageBind: Extending Video-Language
    Pretraining to N-modality by Language-based Semantic
    Alignment**](https://arxiv.org/abs/2310.01852)
18. Ge, et al., 2023, [**Planting a SEED of Vision in Large Language
    Model**](https://arxiv.org/abs/2307.08041)
19. Zhan, et al., 2024, [**AnyGPT: Unified Multimodal LLM with Discrete
    Sequence Modeling**](https://arxiv.org/abs/2402.12226)
20. Kondratyuk, et al., 2023, [**VideoPoet: A Large Language Model for
    Zero-Shot Video Generation**](https://arxiv.org/abs/2312.14125)
21. Zhang, et al., 2023, [**SpeechTokenizer: Unified Speech Tokenizer
    for Speech Large Language
    Models**](https://arxiv.org/abs/2308.16692)
22. Zeghidour, et al., 2021, [**SoundStream: An End-to-End Neural Audio
    Codec**](https://arxiv.org/abs/2107.03312)
23. Liu, et al., 2023, [**Improved Baselines with Visual Instruction
    Tuning**](https://arxiv.org/abs/2310.03744)
24. Wu, et al., 2023, [**Visual-ChatGPT: Talking, Drawing and Editing
    with Visual Foundation Models**](https://arxiv.org/abs/2303.04671)
25. Wang, et al., 2023, [**ModaVerse: Efficiently Transforming
    Modalities with LLMs**](https://arxiv.org/abs/2401.06395)
26. Fei, et al., 2024, [**VITRON: A Unified Pixel-level Vision LLM for
    Understanding, Generating, Segmenting,
    Editing**](http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf)
27. Lu, et al., 2023, [**Unified-IO 2: Scaling Autoregressive Multimodal
    Models with Vision, Language, Audio, and
    Action**](https://arxiv.org/abs/2312.17172)
28. Bai, et al., 2023, [**LVM: Sequential Modeling Enables Scalable
    Learning for Large Vision
    Models**](https://arxiv.org/abs/2312.00785)
29. Huang, et al., 2023, [**Language Is Not All You Need: Aligning
    Perception with Language
    Models**](https://arxiv.org/abs/2302.14045)
30. Li, et al., 2023, [**VideoChat: Chat-Centric Video
    Understanding**](https://arxiv.org/abs/2305.06355)
31. Maaz, et al., 2023, [**Video-ChatGPT: Towards Detailed Video
    Understanding via Large Vision and Language
    Models**](https://arxiv.org/abs/2306.05424)
32. Zhang, et al., 2023, [**Video-LLaMA: An Instruction-tuned
    Audio-Visual Language Model for Video
    Understanding**](https://arxiv.org/abs/2306.02858)
33. Lin, et al., 2023, [**Video-LLaVA: Learning United Visual
    Representation by Alignment Before
    Projection**](https://arxiv.org/abs/2311.10122)
34. Qian, et al., 2024, [**Momentor: Advancing Video Large Language
    Model with Fine-Grained Temporal
    Reasoning**](https://arxiv.org/abs/2402.11435)
35. Hong, et al., 2023, [**3D-LLM: Injecting the 3D World into Large
    Language Models**](https://arxiv.org/abs/2307.12981)
36. Sun, et al., 2023, [**3D-GPT: Procedural 3D Modeling with Large
    Language Models**](https://arxiv.org/abs/2310.12945)
37. Chen, et al., 2023, [**LL3DA: Visual Interactive Instruction Tuning
    for Omni-3D Understanding, Reasoning, and
    Planning**](https://arxiv.org/abs/2311.18651)
38. Xu, et al., 2023, [**PointLLM: Empowering Large Language Models to
    Understand Point Clouds**](https://arxiv.org/abs/2308.16911)
39. Chen, et al., 2024, [**SpatialVLM: Endowing Vision-Language Models
    with Spatial Reasoning
    Capabilities**](https://arxiv.org/abs/2401.12168)
40. Huang, et al., 2023, [**AudioGPT: Understanding and Generating
    Speech, Music, Sound, and Talking
    Head**](https://arxiv.org/abs/2304.12995)
41. Zhang, et al., 2023, [**SpeechGPT: Empowering Large Language Models
    with Intrinsic Cross-Modal Conversational
    Abilities**](https://arxiv.org/abs/2305.11000)
42. Wang, et al., 2023, [**VioLA: Unified Codec Language Models for
    Speech Recognition, Synthesis, and
    Translation**](https://arxiv.org/abs/2305.16107)
43. Rubenstein, et al., 2023, [**AudioPaLM: A Large Language Model That
    Can Speak and Listen**](https://arxiv.org/abs/2306.12925)
44. Tang, et al., 2023, [**SALMONN: Towards Generic Hearing Abilities
    for Large Language Models**](https://arxiv.org/abs/2310.13289)
45. Latif, et al., 2023, [**Sparks of Large Audio Models: A Survey and
    Outlook**](https://arxiv.org/abs/2310.13289)
46. Luo, et al., 2022, [**BioGPT: Generative Pre-trained Transformer for
    Biomedical Text Generation and
    Mining**](https://arxiv.org/abs/2210.10341)
47. Li, et al., 2023, [**DrugGPT: A GPT-based Strategy for Designing
    Potential Ligands Targeting Specific
    Proteins**](https://doi.org/10.1101/2023.06.29.543848)
48. Chen, et al., 2023, [**MEDITRON-70B: Scaling Medical Pretraining for
    Large Language Models**](https://arxiv.org/abs/2311.16079)
49. Wang, et al., 2023, [**HuaTuo: Tuning LLaMA Model with Chinese
    Medical Knowledge**](https://arxiv.org/abs/2304.06975)
50. Zhang, et al., 2023, [**AlpaCare:Instruction-tuned Large Language
    Models for Medical Application**](https://arxiv.org/abs/2310.14558)
51. Frey, et al., 2023, [**Neural Scaling of Deep Chemical
    Models**](https://www.nature.com/articles/s42256-023-00740-3)
52. Zhang, et al., 2023, [**ChemLLM: A Chemical Large Language
    Model**](https://arxiv.org/abs/2402.06852)
53. Liu, et al., 2023, [**MolCA: Molecular Graph-Language Modeling with
    Cross-Modal Projector and Uni-Modal
    Adapter**](https://arxiv.org/abs/2310.12798)
54. Jiang, et al., 2023, [**StructGPT: A General Framework for Large
    Language Model to Reason on Structured
    Data**](https://arxiv.org/abs/2305.09645)
55. Chen, et al., 2024, [**LLaGA: Large Language and Graph
    Assistant**](https://arxiv.org/abs/2402.08170)
56. Koh, et al., 2023, [**Generating Images with Multimodal Language
    Models**](https://arxiv.org/abs/2305.17216)
57. Sun, et al., 2023, [**Generative Pretraining in
    Multimodality**](https://arxiv.org/abs/2307.05222)
58. Zheng, et al., 2023, [**MiniGPT-5: Interleaved Vision-and-Language
    Generation via Generative
    Vokens**](https://arxiv.org/abs/2310.02239)
59. Dong, et al., 2023, [**DreamLLM: Synergistic Multimodal
    Comprehension and Creation**](https://arxiv.org/abs/2309.11499)
60. Liu, et al., 2023, [**LLaVA-Plus: Learning to Use Tools for Creating
    Multimodal Agents**](https://arxiv.org/abs/2311.05437)
61. Wang, et al., 2023, [**GPT4Video: A Unified Multimodal Large
    Language Model for lnstruction-Followed Understanding and
    Safety-Aware Generation**](https://arxiv.org/abs/2311.16511)
62. Jin, et al., 2024, [**Video-LaVIT: Unified Video-Language
    Pre-training with Decoupled Visual-Motional
    Tokenization**](https://arxiv.org/abs/2402.03161)
63. Jin, et al., 2023, [**Chat-UniVi: Unified Visual Representation
    Empowers Large Language Models with Image and Video
    Understanding**](https://arxiv.org/abs/2311.08046)
64. Li, et al., 2023, [**LLaMA-VID: An Image is Worth 2 Tokens in Large
    Language Models**](https://arxiv.org/abs/2311.17043)
65. Su, et al., 2023, [**PandaGPT: One Model to Instruction-Follow Them
    All**](https://arxiv.org/abs/2305.16355)
66. Lyu, et al., 2023, [**Macaw-LLM: Multi-Modal Language Modeling with
    Image, Audio, Video, and Text
    Integration**](https://arxiv.org/abs/2306.09093)
67. Tang, et al., 2023, [**CoDi-2: In-Context, Interleaved, and
    Interactive Any-to-Any
    Generation**](https://arxiv.org/abs/2311.18775)
68. Zhang, et al., 2023, [**GPT4RoI: Instruction Tuning Large Language
    Model on Region-of-Interest**](https://arxiv.org/abs/2307.03601)
69. Yuan, et al., 2023, [**Osprey: Pixel Understanding with Visual
    Instruction Tuning**](https://arxiv.org/abs/2312.10032)
70. Rasheed, et al., 2023, [**GLaMM: Pixel Grounding Large Multimodal
    Model**](https://arxiv.org/abs/2311.03356)
71. Pi, et al., 2023, [**DetGPT: Detect What You Need via
    Reasoning**](https://arxiv.org/abs/2305.14167)
72. Ren, et al., 2023, [**PixelLM: Pixel Reasoning with Large Multimodal
    Model**](https://arxiv.org/abs/2312.02228)
73. Lai, et al., 2023, [**Lisa: Reasoning segmentation via large
    language model**](https://arxiv.org/abs/2308.00692)
74. Chen, et al., 2023, [**Shikra: Unleashing Multimodal LLM\'s
    Referential Dialogue Magic**](https://arxiv.org/abs/2306.15195)
75. Munasinghe, et al., 2023, [**PG-Video-LLaVA: Pixel Grounding in
    Large Multimodal Video Models**](https://arxiv.org/abs/2311.13435)
76. Yu, et al., 2023, [**Merlin: Empowering Multimodal LLMs with
    Foresight Minds**](https://arxiv.org/abs/2312.00589)
77. Fu, et al., 2023, [**MME: A Comprehensive Evaluation Benchmark for
    Multimodal Large Language
    Models**](https://arxiv.org/abs/2306.13394)
78. Xu, et al., 2023, [**LVLM-eHub: A Comprehensive Evaluation Benchmark
    for Large Vision-Language
    Models**](https://arxiv.org/abs/2306.09265)
79. Ying, et al., 2024, [**MMT-Bench: A Comprehensive Multimodal
    Benchmark for Evaluating Large Vision-Language Models Towards
    Multitask AGI**](https://arxiv.org/abs/2404.16006)
80. Pan, et al., 2024, [**Auto-Encoding Morph-Tokens for Multimodal
    LLM**](https://arxiv.org/abs/2405.01926)
81. Thagard, et al., 1997, [**Abductive reasoning: Logic, visual
    thinking, and
    coherence**](https://link.springer.com/chapter/10.1007/978-94-017-0487-8_22)
82. Bavishi, et al., 2023, [**Fuyu-8B: A Multimodal Architecture for AI
    Agents**](https://www.adept.ai/blog/fuyu-8b)




#### Section II: Instruction Tuning & Hallucination 

1.  Liu, et al., 2023, [**Visual Instruction
    Tuning**](https://arxiv.org/abs/2304.08485)
2.  Liu, et al., 2023, [**Mitigating Hallucination in Large Multi-Modal
    Models via Robust Instruction
    Tuning**](https://arxiv.org/abs/2306.14565)
3.  Gao, et al., 2023, [**LLaMA-Adapter V2: Parameter-Efficient Visual
    Instruction Model**](https://arxiv.org/abs/2304.15010)
4.  Zhao, et al., 2023, [**SVIT: Scaling up Visual Instruction
    Tuning**](https://arxiv.org/abs/2307.04087)
5.  Ye, et al., 2023, [**mPLUG-Owl: Modularization Empowers Large
    Language Models with
    Multimodality**](https://arxiv.org/abs/2304.14178)
6.  Yu, et al., 2023, [**RLHF-V: Towards Trustworthy MLLMs via Behavior
    Alignment from Fine-grained Correctional Human
    Feedback**](https://arxiv.org/abs/2312.00849)
7.  Liu, et al., 2023, [**MMC: Advancing Multimodal Chart Understanding
    with Large-scale Instruction
    Tuning**](https://arxiv.org/abs/2311.10774)
8.  Zhao, et al., 2023, [**MiniGPT-4: Enhancing Vision-Language
    Understanding with Advanced Large Language
    Models**](https://arxiv.org/abs/2304.10592)
9.  Liu, et al., 2023, [**HallusionBench: You See What You Think? Or You
    Think What You See? An Image-Context Reasoning Benchmark Challenging
    for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality
    Models**](https://arxiv.org/abs/2310.14566)
10. Li, et al., 2023, [**Evaluating Object Hallucination in Large
    Vision-Language Models**](https://arxiv.org/abs/2305.10355)
11. Huang, et al., 2023, [**Visual Instruction Tuning towards
    General-Purpose Multimodal Model: A
    Survey**](https://arxiv.org/abs/2306.13549)
12. Yin, et al., 2023, [**A Survey on Multimodal Large Language
    Models**](https://arxiv.org/abs/2312.16602)
13. Yin, et al., 2023, [**Woodpecker: Hallucination Correction for
    Multimodal Large Language
    Models**](https://arxiv.org/abs/2312.16602)


#### Section III: Reasoning with LLM 

1.  Zhang, et al., 2023, [**Multimodal Chain-of-Thought Reasoning in
    Language Models**](https://arxiv.org/abs/2302.00923)
2.  Zhao, et al., 2023, [**MMICL: Empowering Vision-language Model with
    Multi-Modal In-Context
    Learning**](https://arxiv.org/abs/2309.07915)
3.  Lu, et al., 2023, [**Chameleon: Plug-and-Play Compositional
    Reasoning with Large Language
    Models**](https://arxiv.org/abs/2304.09842)
4.  Zhang, et al., 2023, [**You Only Look at Screens: Multimodal
    Chain-of-Action Agents**](https://arxiv.org/abs/2309.11436)
5.  Sun, et al., 2023, [**Generative multimodal models are in-context
    learners**](https://arxiv.org/abs/2312.13286)
6.  Fei, et al., 2023, [**VITRON: A Unified Pixel-level Vision LLM for
    Understanding, Generating, Segmenting,
    Editing**](https://vitron-llm.github.io/)
7.  Wei, et al., 2023, [**Enhancing Human-like Multi-Modal Reasoning: A
    New Challenging Dataset and Comprehensive
    Framework**](https://arxiv.org/abs/2307.12626)
8.  Zhang, et al., 2023, [**Igniting Language Intelligence: The
    Hitchhiker\'s Guide From Chain-of-Thought Reasoning to Language
    Agents**](https://arxiv.org/abs/2311.11797)
9.  Fei, et al., 2024, [**Video-of-Thought: Step-by-Step Video Reasoning
    from Perception to Cognition**](http://haofei.vip/VoT/)
10. Prystawski, et al., 2023, [**Why think step by step? Reasoning
    emerges from the locality of
    experience**](https://arxiv.org/abs/2304.03843)
11. Gou, et al., 2023, [**CRITIC: Large Language Models Can Self-Correct
    with Tool-Interactive
    Critiquing**](https://arxiv.org/abs/2305.11738)
12. Tang, et al., 2024, [**Prioritizing Safeguarding Over Autonomy:
    Risks of LLM Agents for
    Science**](https://arxiv.org/abs/2402.04247)
13. Yuan, et al., 2024, [**R-Judge: Benchmarking Safety Risk Awareness
    for LLM Agents**](https://arxiv.org/abs/2401.10019)




#### Section IV: Efficient Learning 

1.  Hu, et al., 2021, [**LoRA: Low-Rank Adaptation of Large Language
    Models**](https://arxiv.org/abs/2106.09685)
2.  Dettmers, et al., 2023, [**QLoRA: Efficient Finetuning of Quantized
    LLMs**](https://arxiv.org/abs/2305.14314)
3.  Li, et al., 2023, [**BLIP-2: Bootstrapping Language-Image
    Pre-training with Frozen Image Encoders and Large Language
    Models**](https://arxiv.org/abs/2301.12597)
4.  Luo, et al., 2023, [**Cheap and Quick: Efficient Vision-Language
    Instruction Tuning for Large Language
    Models**](https://arxiv.org/abs/2305.15023)
5.  Yao, et al., 2024,
    [**MiniCPM-V**](https://github.com/OpenBMB/MiniCPM-V)
6.  DeepSpeed Team, 2020, [**DeepSpeed
    Blog**](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
7.  Zhao, et al., 2023, [**PyTorch FSDP: Experiences on Scaling Fully
    Sharded Data Parallel**](https://arxiv.org/abs/2304.11277)
8.  Zhu, et al., 2023, [**MiniGPT-4: Enhancing Vision-Language
    Understanding with Advanced Large Language
    Models**](https://arxiv.org/abs/2304.10592)
9.  Chen, et al., 2023, [**MiniGPT-v2: large language model as a unified
    interface for vision-language multi-task
    learning**](https://arxiv.org/abs/2310.09478)
10. Hong, et al., 2023, [**CogAgent: A Visual Language Model for GUI
    Agents**](https://arxiv.org/abs/2312.08914)
11. Chen, et al., 2024, [**How Far Are We to GPT-4V? Closing the Gap to
    Commercial Multimodal Models with Open-Source
    Suites**](https://arxiv.org/abs/2404.16821)
12. Dehghani, et al., 2023, [**Patch n\' Pack: NaViT, a Vision
    Transformer for any Aspect Ratio and
    Resolution**](https://arxiv.org/abs/2307.06304)
13. Zhang, et al., 2023, [**VPGTrans: Transfer Visual Prompt Generator
    across LLMs**](https://arxiv.org/abs/2305.01278)
14. Wu, et al., 2023, [**NExT-GPT: Any-to-Any Multimodal
    LLM**](https://arxiv.org/abs/2309.05519)
15. Fei, et al., 2024, [**VITRON: A Unified Pixel-level Vision LLM for
    Understanding, Generating, Segmenting,
    Editing**](http://haofei.vip/downloads/papers/Skywork_Vitron_2024.pdf)
16. Zhang, et al., 2024, [**NExT-Chat: An LMM for Chat, Detection and
    Segmentation**](https://arxiv.org/abs/2311.04498)