---
layout: post
title: Deep Learning Theory Lectures
date: 2022-07-19
categories: [computer science]
tags: [machine learning, graph mining]

---


## Article Source
* [Lectures](https://deeplearningtheory.com/lectures/)

---



# Deep Learning Theory Lectures

An overview of the material in *[The Principles of Deep Learning
Theory](..)* (PDLT) was given virtually in 2021 at the [Princeton Deep
Learning Theory](https://deep-learning-summer-school.princeton.edu)
(PDLT) Summer School by [Dan Roberts](https://danintheory.com) and [Sho
Yaida](https://shoyaida.com).

### Lecture 1

-   [Slides](../Princeton-School/Lecture-1.pdf)
-   [YouTube](https://youtube.com/watch?v=PRyEApLxcSQ&t=5430s)

> TL;DW Dan gives an overview of the course and begins a discussion of
> training dynamics by covering linear models and kernel methods.

### Lecture 2

-   [Slides](../Princeton-School/Lecture-2.pdf)
-   [YouTube](https://youtube.com/watch?v=OBrjyorRCxo&t=0s)

> TL;DW Dan introduces the quadratic model as a minimal model of
> representation learning, and use gradient descent to solve the
> training dynamics. This extends kernel methods to "nearly-kernel
> methods."

### Lecture 3

-   [Slides](../Princeton-School/Lecture-3.pdf)
-   [YouTube](https://youtube.com/watch?v=sS4vvt4dp5U&t=0s)

> TL;DW Sho explains how to recursively compute the statistics of a deep
> and finite-width MLP at initialization. Due to the principle of
> sparsity, the distribution of the network output is tractable.

### Lecture 4

-   [Slides](../Princeton-School/Lecture-4.pdf)
-   [YouTube](https://youtube.com/watch?v=0BUl_5cMilM&t=10190s)

> TL;DW Sho solves the layer-to-layer recursions derived before with the
> principle of criticality. We learn that the leading finite-width
> effects scale like the depth-to-width ratio of the network.

### Lecture 5

-   [Slides](../Princeton-School/Lecture-5.pdf)
-   [YouTube](https://youtube.com/watch?v=dM1PDZ8Bgkw&t=5338s)

> TL;DW By combining init statistics & training dynamics we get
> [this](https://twitter.com/danintheory/status/1405913074087960578?s=20).
> Then, Dan explain how MLPs \*-polate, how to estimate a network's
> optimal aspect ratio, and how to think about complexity.
