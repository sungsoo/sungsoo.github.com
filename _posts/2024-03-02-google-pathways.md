---
layout: post
title: Pathways Language Model and Model Scaling
date: 2024-03-02
categories: [artificial intelligence]
tags: [machine learning]

---

### Article Source


* [Pathways Language Model and Model Scaling](https://www.youtube.com/watch?v=CV_eBVwzOaw)

---

# Pathways Language Model and Model Scaling


* Speaker: Aakanksha Chowdhery 


## Abstract

Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, dense Transformer language model at Google, which we refer to as Pathways Language Model (PaLM). In this talk, we discuss the system considerations and model improvements necessary to train the PaLM model across 6144 TPU v4 chips using Pathways at very high efficiency levels. Next we share how scaling the model to 540B parameters results in state-of-the-art few shot learning results across hundreds of language understanding and generation benchmarks. We will also share some of the more recent works built on top of PaLM that push the SOTA in various domains and democratize access to natural language processing.

대규모 언어 모델은 몇 가지 예시만으로 학습하는 "few-shot learning" 기법을 사용하여 다양한 자연어 처리 작업에서 놀라운 성능을 보여주었습니다. 이 기법은 모델을 특정 응용 프로그램에 적용하는 데 필요한 작업별 훈련 예시의 수를 획기적으로 줄여줍니다. 규모가 few-shot learning에 미치는 영향을 더 잘 이해하기 위해 Google에서는 5400억 개의 매개변수를 가진 밀집 트랜스포머 언어 모델을 훈련했습니다. 이 모델을 Pathways 언어 모델(PaLM)이라고 부릅니다. 이 발표에서는 Pathways를 사용하여 6144개의 TPU v4 칩에서 매우 높은 효율 수준으로 PaLM 모델을 훈련하는 데 필요한 시스템 고려사항과 모델 개선 사항에 대해 논의합니다. 다음으로 모델을 5400억 개의 매개변수로 확장하면 수백 개의 언어 이해 및 생성 벤치마크에서 최첨단 Few-shot learning 결과를 얻을 수 있음을 설명합니다. 또한 PaLM을 기반으로 다양한 분야에서 최첨단 성능을 끌어올리고 자연어 처리 접근성을 민주화하는 최근 연구 몇 가지도 공유할 예정입니다.

<iframe width="600" height="400" src="https://www.youtube.com/embed/CV_eBVwzOaw?si=sdbi9B2DGDnYrG1-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## Bio
Aakanksha has led the effort on training large language models at Google Research which led to the 540B PaLM model. Aakanksha has also been a core member of the Pathways project at Google. Prior to joining Google, Aakanksha led interdisciplinary teams at Microsoft Research and Princeton University across machine learning, distributed systems and networking.  Aakanksha completed her PhD in Electrical Engineering from Stanford University, and was awarded the Paul Baran Marconi Young Scholar Award for the outstanding scientific contributions in the field of communications and the Internet.
