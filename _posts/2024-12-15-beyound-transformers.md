---
layout: post
title:  Beyound Transformers
date: 2024-12-15
categories: [artificial intelligence]
tags: [machine learning]

---

# Beyound Transformers

# 트랜스포머를 넘어: 새로운 아키텍처 탐색

## 트랜스포머의 한계 극복을 위한 다양한 시도

범용적인 트랜스포머 아키텍처의 막대한 성공에도 불구하고, 많은 연구자들은 트랜스포머의 단점을 극복하고 더 나은 성능을 내는 새로운 아키텍처를 찾기 위해 노력해왔습니다.

## 전문가 혼합 모델 (Mixture of Experts, MoEs)

MoE는 트랜스포머 모델의 조밀한 레이어를 여러 개의 "전문가" 서브 네트워크로 구성된 조건부 모듈로 대체합니다. 각 토큰이나 태스크마다 어떤 전문가를 사용할지 동적으로 결정하는 라우팅 메커니즘을 사용합니다. MoE는 모델 크기가 같더라도 더 빠르게 학습하고 디코딩할 수 있으며, 각 전문가가 서로 다른 추상적인 태스크에 특화될 수 있다는 장점이 있습니다. 하지만 모든 전문가를 VRAM에 로드하고 여러 노드에 분산시켜야 하는 등의 시스템적인 문제점도 존재합니다.

## 상태 공간 모델 (State Space Models, SSMs)

SSMs는 시퀀스 투 시퀀스 변환을 모델링하는 데 사용되며, 트랜스포머의 2차원적인 자기 주의 메커니즘을 대체할 수 있습니다. SSM은 각 시간 단계(토큰)에서 (A, B, C)라는 학습 가능한 파라미터 튜플을 통해 재귀 관계를 정의합니다. SSM의 주요 과제는 이러한 재귀 관계를 병렬 처리하여 현대 하드웨어 가속기를 효율적으로 활용하는 방법을 찾는 것입니다. 선형 어텐션은 SSM의 간단한 형태로 볼 수 있으며, 커널 특징 맵의 선형 점곱을 통해 자기 어텐션을 표현하여 계산 복잡도를 줄입니다. S4는 SSM을 저랭크 보정을 통해 효율적이고 표현력 있게 파라미터화하여 안정적으로 대각화하고 코시 커널 계산으로 축소합니다. 이후 많은 연구들이 SSM의 전이 행렬 A를 다르게 파라미터화하여 계산 효율성과 모델링 성능을 개선하려는 시도를 해왔습니다. H3는 이전 토큰을 회상하고 시퀀스 간 토큰 비교를 지원하도록 특별히 설계된 두 개의 별개 SSM으로 구성된 SSM 블록을 제안합니다. Hyena는 S4 레이어를 인터리빙되고 암묵적으로 파라미터화된 긴 합성곱과 데이터 제어 게이팅으로 대체하여 필터 크기와 무관하게 파라미터 크기를 조절할 수 있도록 함으로써 더 큰 표현력을 제공합니다. Retentive Network는 추가적인 게이트를 포함하고 다중 헤드 어텐션의 변형을 사용하여 일정한 추론 비용과 선형적인 긴 시퀀스 메모리 소비량을 달성합니다. RWKV는 트랜스포머의 효율적인 병렬 학습과 RNN의 효율적인 추론을 결합한 새로운 아키텍처입니다. Mamba는 SSM의 콘텐츠 기반 추론 능력 부족 문제를 해결하기 위해 선택적인 상태 공간을 제안합니다.

## RNN의 부활

긴 컨텍스트 처리에 강점을 가진 RNN에 대한 관심이 다시 높아지고 있습니다. Hawk은 게이티드 선형 재귀를 사용하는 RNN 기반 모델이며, Griffin은 게이티드 선형 재귀와 지역 어텐션을 혼합합니다. 이들은 Mamba보다 다운스트림 태스크에서 우수한 성능을 보여주었으며, Llama-2와 동등한 성능을 6배 적은 학습 토큰으로 달성했습니다. Recurrent Gemma는 다양한 모델 크기로 사전 학습되고 지시 조정된 버전을 제공하며, 데이터 효율적이고 고정된 상태 크기를 가지며 긴 컨텍스트를 처리할 수 있는 모델을 트랜스포머 아키텍처에 의존하지 않고 학습할 수 있는 가능성을 보여줍니다.

## 아키텍처 하이브리드화

최근 연구들은 트랜스포머와 Mamba를 결합하여 각 아키텍처의 장점을 취하려는 시도를 하고 있습니다. MAD는 소규모 성능 단위 테스트를 기반으로 스케일링 법칙을 예측하는 엔드 투 엔드 파이프라인을 통해 효율적인 아키텍처를 찾는 방법을 제안합니다. MAD는 Transformer++, Hyena, Mamba를 능가하는 성능을 보이는 Striped Hyena라는 효율적인 아키텍처를 찾아냈습니다.

## 결론

트랜스포머를 넘어 다양한 아키텍처를 탐색하는 연구가 활발하게 진행되고 있습니다. MoE, SSM, RNN 등 다양한 아키텍처의 장점을 결합하고, 하이브리드화를 통해 더욱 효율적이고 강력한 모델을 개발하려는 시도가 계속될 것입니다. 이러한 연구들은 컴퓨터 비전, 자연어 처리 등 다양한 분야에 긍정적인 영향을 미칠 것으로 기대됩니다.

**핵심 키워드:** 트랜스포머, MoE, SSM, RNN, 하이브리드 아키텍처, 딥러닝, 자연어 처리


